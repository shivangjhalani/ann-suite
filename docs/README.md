# ANN Benchmarking Suite Documentation

Welcome to the ANN Benchmarking Suite documentation. This suite provides production-grade benchmarking for Approximate Nearest Neighbor (ANN) algorithms with containerized isolation and deep observability.

## Documentation Index

| Document | Description |
|----------|-------------|
| [Architecture](./ARCHITECTURE.md) | Internal workings, components, and data flow |
| [Configuration Reference](./CONFIGURATION.md) | Complete YAML configuration options |
| [Adding Algorithms](./ADDING_ALGORITHMS.md) | How to add new algorithm implementations |
| [Adding Datasets](./ADDING_DATASETS.md) | How to add and manage datasets |
| [Metrics Reference](./METRICS.md) | All metrics collected and how they're measured |
| [API Reference](./API.md) | Python API for programmatic usage |
| [Docker Optimizations](./DOCKER_OPTIMIZATIONS.md) | Runtime settings for research-grade performance |

## Quick Links

- **Getting Started**: See the [README](../README.md) for installation and quick start
- **Example Configs**: Check `configs/` directory for working examples
- **Library**: Algorithm and dataset implementations in `library/`

## Features

- ðŸ³ **Containerized Isolation**: Every algorithm runs in its own Docker container
- ðŸ“Š **Deep Observability**: Measures QPS, latency, recall, RAM, and Disk IOPS
- ðŸ’¾ **Storage Modes**: Evaluates both in-memory (HNSW) and disk-based (DiskANN) algorithms
- âš™ï¸ **Modular & Configurable**: YAML/JSON configs, pluggable algorithms, extensible metrics

## Requirements

> [!IMPORTANT]
> **cgroups v2 is required** for running benchmarks. The suite will fail at startup if cgroups v2 is not available.

Verify with:
```bash
cat /sys/fs/cgroup/cgroup.controllers
# Should output: cpuset cpu io memory hugetlb pids rdma misc
```

See [METRICS.md](./METRICS.md#requirements) for setup instructions if cgroups v2 is not enabled.

## Project Structure

```
ann-suite/
â”œâ”€â”€ src/ann_suite/            # Core benchmarking framework
â”‚   â”œâ”€â”€ core/                 # Schemas (Pydantic models), config loading
â”‚   â”‚   â”œâ”€â”€ schemas.py        # BenchmarkConfig, AlgorithmConfig, *Metrics
â”‚   â”‚   â””â”€â”€ config.py         # YAML/JSON loading and validation
â”‚   â”œâ”€â”€ monitoring/           # Resource monitoring via cgroups v2
â”‚   â”‚   â”œâ”€â”€ base.py           # BaseCollector abstract class
â”‚   â”‚   â””â”€â”€ cgroups_collector.py  # CgroupsV2Collector implementation
â”‚   â”œâ”€â”€ runners/              # Docker container lifecycle
â”‚   â”‚   â””â”€â”€ container_runner.py   # ContainerRunner with metrics
â”‚   â”œâ”€â”€ datasets/             # Dataset loading utilities
â”‚   â”œâ”€â”€ results/              # Result storage (JSON, CSV)
â”‚   â”‚   â””â”€â”€ storage.py        # ResultsStorage class
â”‚   â”œâ”€â”€ evaluator.py          # BenchmarkEvaluator pipeline
â”‚   â””â”€â”€ cli.py                # Typer CLI (run, build, report, download)
â”œâ”€â”€ library/                  # Algorithm & dataset library
â”‚   â”œâ”€â”€ algorithms/           # Algorithm implementations (HNSW, DiskANN)
â”‚   â”‚   â”œâ”€â”€ hnsw/             # HNSW container (Dockerfile + runner.py)
â”‚   â”‚   â”œâ”€â”€ diskann/          # DiskANN container
â”‚   â”‚   â””â”€â”€ utils.py          # Shared utilities (compute_recall, etc.)
â”‚   â””â”€â”€ datasets/             # Dataset registry and download utilities
â”œâ”€â”€ configs/                  # Benchmark configuration files (YAML)
â”œâ”€â”€ docs/                     # This documentation
â””â”€â”€ tests/                    # Test suite (pytest)
```
